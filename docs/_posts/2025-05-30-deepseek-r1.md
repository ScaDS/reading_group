---
layout: post
title: DeepSeek-R1
date: 2025-05-30 13:00
categories: grpo r1
---


## Presented by Andrei Politov and Lalith Manjunath

<video width="560" height="315" controls>
  <source src="https://cloud.scadsai.uni-leipzig.de/remote.php/webdav/dd-scads-ml-reading-group/Videos/ml-reading-group-R1-30-05-25.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

> ‚ùóThis summary was generated by an LLM.

**Meeting Summary: DeepSeek-R1 Paper Review**

**Overview:**

The meeting centers around a presentation and discussion of the DeepSeek-R1 paper, titled "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning". The presentation is split, with Andrei focusing on the approach and Lalith on experimental results. The key focus is on how DeepSeek achieves impressive reasoning capabilities in large language models (LLMs) using reinforcement learning (RL), and how they compare to other models like GPT and Gemini.

**Andre\'s Presentation: Approach & Training**
*   **Introduction to DeepSeek-R1:**
    *   A reasoning model developed in China, showcasing effective use of existing research.
    *   Achieves high performance relative to the investment in OpenAI/Microsoft models.
    *   Potentially leverages H800 GPUs (Chinese version of H100) due to access limitations.

*   **Key Insight: SFT (Supervised Fine-Tuning) is not always essential:**
    *   DeepSeek-V3 is used as the base model (which already has pre-training, supervised fine-tuning and reinforcement learning).
    *   Reinforcement learning is what is key.

*   **Two Models: R1-0 and R1:**
    *   R1-0 faced reliability issues. R1 is an improved iteration.
    *   R1-0 serves as inspiration for R1, and its output is used for distillation.

*   **RL without SFT:**
    *   The team directly applies RL to the base model (V3) without needing a preliminary SFT step.

*   **R1-0 Details:**
    *   Employs Grouped Relative Policy Optimization (GRPO). This avoids using a critic model in the pipeline.
    *   Uses a rule-based reward model, not neural network-based. Rewards are based on accuracy and formatting.
    *   Formats the model to "think" first and then "answer."

*   **Chain of Thought (CoT):** The models use a straightforward chain of thought, prompting the model to "think" before providing the answer.

*   **Performance Evolution:** The model shows improvements in accuracy during training.

*   **Aha Moment**: Highlights the Aha moment during training with the model thinking solving quadratic equation.

*   **Average Response Length:** The average response length increases during training, indicating that the model is learning to "think" and reflect more.

*   **R1 Improvements:**
    *   Addresses issues in R1-0, specifically language mixing and chaotic outputs.
    *   Incorporates "cold start" data: high-quality, long CoT examples used for fine-tuning before reinforcement learning.
    *   Cold start data is produced by deep seek r1 zero and post processed by human annotators.
    *   Fine-tunes DeepSeek-V3 with cold start data, then uses the same RL process as R1-0.
    *   Implements a language consistency reward to penalize language mixing.
    *   Uses Rejection sampling and SFT and SFT data is from different domains to improve the things like writing, role playing, and other general purpose tasks.
    *   Uses Rule Based Rewards and a Reward Model from DeepSeek V3 for helpfulness and harmlessness.
    
*   **Distillation:**
    *   Distills R1 into smaller open-source models (like Llama) via SFT.
    *   Distillation process leverages the outputs of R1.

**Lalith\'s Presentation: Experiments & Context**

*   **GRPO Loss Function Explanation:** 
    Lalith defines GRPO in simple terms, emphasizing its optimization over a *group* of outputs rather than individual data points.

*   **Evaluation:** R1 achieves good evaluation scores approaching cloud and OpenAI models. Notes the evaluation benchmarks are adapted, by changing prompts and cutting off the length of generations.

*   **Parsat@1:** The evaluation metric favours the GRPO set of evals.

*   **R1 & Length:** R1\'s performance relies on the length of its output, which may be limited by standard evaluation frameworks.

*   **Political Context**: Stresses DeepSeek transparency for the purposes of surveillance.

*   **Political Context**: China is prioritizing transparent and trustworthy AI models, especially in the context of AI model surveillance.

*   **Alignment with OpenAI/Cloud:** Clusters responses based on embeddings, reveals R1 is in line with responses from OpenAI.

*   **Resource Limitations:** Due to resource limits, R1\'s training is based on the API services of OpenAI and cloud. DeepSeek is transitioning to use Gemini instead of OpenAI/Cloud.

*   **Alternative RL Techniques:** PRM is a chicken and egg problem, and Monte Carlo is computationally infeasible.

*   **Revisiting Distillation:** Discusses biases of the GRPO loss function towards longer outputs, which are addressed in Dr. GRPO.
    *   Instead of RL in smaller models, use SFT on the data collected from the larger model. This is because RL gradient updates only affect very specific parameters in the whole network.
    *   **Aha Moments:** The "aha moment" that DeepSeek repeatedly highlights in their paper is the effect of the prompt template itself, and is already present in the base model, and not a result of RL.
        
**Discussion Points**
*   **Output Length:**  Participants discuss whether longer answers are actually preferred by users. Many find overly verbose responses annoying and prefer concise answers. It is a bias in the RL objective.
*   **Intentional Verbosity:** There is a discussion about whether the longer responses from models like ChatGPT are intentional, serving to improve the overall context and, therefore, the final answer.
